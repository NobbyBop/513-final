{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dae31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imports here\n",
    "!pip install imbalanced-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and \"cleaning\" data\n",
    "df = pd.read_csv(\"California-Wildfire-Data.csv\", low_memory=False)\n",
    "df_obj = df.select_dtypes(\"object\")\n",
    "\n",
    "df_obj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric data\n",
    "df_num = df.select_dtypes(\"number\")\n",
    "\n",
    "df_num[\"* Street Number\"] = df_num[\"* Street Number\"].replace(to_replace=0, value=np.nan)\n",
    "print(f\"Missing values in Street Number: {df_num[\"* Street Number\"].isna().sum()}\")\n",
    "\n",
    "df_num[\"Assessed Improved Value (parcel)\"] = df_num[\"Assessed Improved Value (parcel)\"].replace(to_replace=0, value=np.nan)\n",
    "print(f\"Missing values in Assessed Improved Value (parcel): {df_num[\"Assessed Improved Value (parcel)\"].isna().sum()}\")\n",
    "\n",
    "df_num[\"Year Built (parcel)\"] = df_num[\"Year Built (parcel)\"].replace(to_replace=0, value=np.nan)\n",
    "print(f\"Missing values in Year Built (parcel): {df_num[\"Year Built (parcel)\"].isna().sum()}\")\n",
    "\n",
    "#df_num = df_num.dropna()\n",
    "#print(f\"Rows remaining after dropping na: {len(df_num.index)}\")\n",
    "\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0df336",
   "metadata": {},
   "source": [
    "# Exploritory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zus7w80qn8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "for col in df_obj.columns:\n",
    "    unique_vals = df_obj[col].unique()\n",
    "    n_unique = len(unique_vals)\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Number of unique values: {n_unique}\")\n",
    "    if \"unknown\" in list(unique_vals):\n",
    "        print(f\"Number of missing values: {0 + df_obj[col].value_counts()[\"unknown\"]}\")\n",
    "    if \"Unknown\" in list(unique_vals):\n",
    "        print(f\"Number of missing values: {0 + df_obj[col].value_counts()[\"Unknown\"]}\")\n",
    "    if n_unique <= 20:\n",
    "        print(f\"Values: {list(unique_vals)}\")\n",
    "    else:\n",
    "        print(f\"Sample values (first 20): {list(unique_vals[:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_num.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Mean: {df_num[col].mean():.2f}\")\n",
    "    print(f\"Median: {df_num[col].median():.2f}\")\n",
    "    mode_vals = df_num[col].mode()\n",
    "    if len(mode_vals) > 0:\n",
    "        print(f\"Mode: {mode_vals[0]:.2f}\")\n",
    "    else:\n",
    "        print(f\"Mode: N/A\")\n",
    "    print(f\"Std Dev: {df_num[col].std():.2f}\")\n",
    "    print(f\"Min: {df_num[col].min():.2f}\")\n",
    "    print(f\"Max: {df_num[col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1520d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bf7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_encoded = df_obj.copy()\n",
    "le_target = LabelEncoder()\n",
    "\n",
    "# Fit and save the target encoder\n",
    "le_target.fit(df_obj['* Damage'].astype(str))\n",
    "\n",
    "# Encode all columns\n",
    "for col in df_encoded.columns:\n",
    "    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col].astype(str))\n",
    "\n",
    "# combine numeric and encoded categorical data, not feature selected yet\n",
    "attr = pd.concat([df_encoded.drop(['* Damage'], axis=1), df_num], axis=1)\n",
    "target = df_encoded['* Damage']\n",
    "\n",
    "attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=15)\n",
    "# optimal attributes/features\n",
    "out_feats = selector.fit_transform(attr.fillna(-1), target)\n",
    "opt_attr = pd.DataFrame(out_feats, columns=selector.get_feature_names_out()).dropna(axis=1)\n",
    "print(\"Best features:\", selector.get_feature_names_out())\n",
    "# Split data into training and testing sets 80-20\n",
    "attr_train, attr_test, target_train, target_test = train_test_split(opt_attr, target, test_size=0.2, random_state=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9682c",
   "metadata": {},
   "source": [
    "# KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Implementation with SMOTE, Scaling and Best Params\n",
    "import os\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '1'\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Apply SMOTE to Training Data Only\n",
    "print(\"Applying SMOTE to training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "attr_train_res, target_train_res = smote.fit_resample(attr_train, target_train)\n",
    "\n",
    "# Round to nearest int to keep categorical nature valid\n",
    "attr_train_res = np.round(attr_train_res).astype(int)\n",
    "\n",
    "# Scaling (Fit on Resampled Training Data)\n",
    "scaler = MinMaxScaler()\n",
    "attr_train_scaled = scaler.fit_transform(attr_train_res)\n",
    "attr_test_scaled = scaler.transform(attr_test)\n",
    "\n",
    "# # Grid Search (Commented out as requested)\n",
    "# print(\"Starting Grid Search for KNN...\")\n",
    "# param_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']}\n",
    "# grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, n_jobs=1, verbose=1)\n",
    "# grid_knn.fit(attr_train_scaled, target_train_res)\n",
    "# print(f\"Best KNN Parameters: {grid_knn.best_params_}\")\n",
    "# target_pred = grid_knn.predict(attr_test_scaled)\n",
    "\n",
    "# Using Best Parameters directly\n",
    "print(\"Training KNN with best parameters (n_neighbors=3, weights='distance')...\")\n",
    "knn = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "knn.fit(attr_train_scaled, target_train_res)\n",
    "target_pred = knn.predict(attr_test_scaled)\n",
    "\n",
    "# Decode for report\n",
    "target_test_decoded = le_target.inverse_transform(target_test)\n",
    "target_pred_decoded = le_target.inverse_transform(target_pred)\n",
    "labels = le_target.classes_\n",
    "\n",
    "# Evaluation\n",
    "print(\"KNN Classification Report (with SMOTE):\")\n",
    "print(classification_report(target_test_decoded, target_pred_decoded))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(target_test_decoded, target_pred_decoded, labels=labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix - KNN (SMOTE + Best Params)')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5977389",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a distance-based classifier that predicts damage by finding the most similar examples in the training set; here, we use SMOTE to balance the classes and `weights='distance'` to prioritize closer neighbors. Our accurary is 83%, but improved recall for minority classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789ef99",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce34b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a label encoder here because using the get_dummies() method takes too much memory and crashes the kernel.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df_encoded = opt_attr.copy()\n",
    "le_target = LabelEncoder()\n",
    "\n",
    "# Fit and save the target encoder\n",
    "le_target.fit(df_obj['* Damage'].astype(str))\n",
    "\n",
    "\n",
    "# Encode all columns\n",
    "for col in df_encoded.columns:\n",
    "    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col].astype(str))\n",
    "\n",
    "# attr = df_encoded.drop(['* Damage'], axis=1)\n",
    "# target = df_encoded['* Damage']\n",
    "\n",
    "attr_train, attr_test, target_train, target_test = train_test_split(attr, target, test_size=0.3, random_state=6)\n",
    "\n",
    "# Added class_weight='balanced' to handle class imbalance\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(attr_train, target_train)\n",
    "target_pred = model.predict(attr_test)\n",
    "\n",
    "# Decode predictions and targets\n",
    "target_test_decoded = le_target.inverse_transform(target_test)\n",
    "target_pred_decoded = le_target.inverse_transform(target_pred)\n",
    "\n",
    "# Get class labels\n",
    "labels = le_target.classes_\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(target_test_decoded, target_pred_decoded, labels=labels)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - CART (Balanced)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(target_test_decoded, target_pred_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc43b2",
   "metadata": {},
   "source": [
    "Using the CART method, we see that our model is 89% accurate. We can also see that most of our data either falls into \"No Damage\" or \"Destroyed\". There is significantly less data for any other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39d58f",
   "metadata": {},
   "source": [
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Implementation (with best parameters from tuning)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import train_test_split  # , GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Encode the data (similar to CART approach)\n",
    "# Use LabelEncoder on each column to convert to non-negative integers\n",
    "df_encoded_nb = opt_attr.copy()\n",
    "\n",
    "for col in df_encoded_nb.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded_nb[col] = le.fit_transform(df_encoded_nb[col].astype(str))\n",
    "\n",
    "# 2. Split Data (80-20 split)\n",
    "attr_train_nb, attr_test_nb, target_train_nb, target_test_nb = train_test_split(\n",
    "    df_encoded_nb, target, test_size=0.2, random_state=6\n",
    ")\n",
    "\n",
    "# 3. Compute sample weights for class balancing\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=target_train_nb)\n",
    "\n",
    "# 4. Calculate min_categories from the full encoded dataset\n",
    "min_categories = [int(df_encoded_nb[col].max() + 1) for col in df_encoded_nb.columns]\n",
    "\n",
    "# # 5. Hyperparameter Tuning with GridSearchCV (COMMENTED OUT - using best params below)\n",
    "# print(\"Performing hyperparameter tuning for CategoricalNB...\")\n",
    "# param_grid = {\n",
    "#     'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "# }\n",
    "# \n",
    "# nb_base = CategoricalNB(min_categories=min_categories)\n",
    "# grid_search = GridSearchCV(\n",
    "#     nb_base, \n",
    "#     param_grid, \n",
    "#     cv=5, \n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "# \n",
    "# grid_search.fit(attr_train_nb, target_train_nb, sample_weight=sample_weights)\n",
    "# \n",
    "# print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "# \n",
    "# nb = grid_search.best_estimator_\n",
    "\n",
    "# 5. Train model with best parameters (from grid search: alpha=1.0)\n",
    "print(\"Training CategoricalNB with best parameters (alpha=1.0)...\")\n",
    "nb = CategoricalNB(alpha=0.01, min_categories=min_categories)\n",
    "nb.fit(attr_train_nb, target_train_nb, sample_weight=sample_weights)\n",
    "\n",
    "# 6. Predict\n",
    "target_pred_nb = nb.predict(attr_test_nb)\n",
    "\n",
    "# 7. Evaluate\n",
    "# Decode targets for readable report\n",
    "target_test_decoded = le_target.inverse_transform(target_test_nb)\n",
    "target_pred_decoded = le_target.inverse_transform(target_pred_nb)\n",
    "\n",
    "print(\"\\nNaive Bayes Classification Report:\")\n",
    "print(classification_report(target_test_decoded, target_pred_decoded))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_nb = confusion_matrix(target_test_decoded, target_pred_decoded, labels=le_target.classes_)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_target.classes_, yticklabels=le_target.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Naive Bayes')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8cd2fa",
   "metadata": {},
   "source": [
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem; we use `CategoricalNB` suitable for our labeled data and apply SMOTE to improve the detection of rare damage categories. Our accurary is 73%, but improved recall for minority classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80bd5e",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d769a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22767b6",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb489530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc540d3",
   "metadata": {},
   "source": [
    "# HClust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedeb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af42bc",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8910f9",
   "metadata": {},
   "source": [
    "# Agentic Model\n",
    "The Agentic model works using a few external technologies:\n",
    "1. Anthropic's Claude LLM for generating Python code\n",
    "2. E2B -  a secure sandbox for running AI generated code.\n",
    "Basically, we instruct the LLM to do data analysis tasks on our data set via generating Python code. Then, we analyze the results of the code and send it back to the LLM for further generation/analysis. Ultimately, we do this in a series of steps starting with EDA, going to cleaning, and then model picking and running. Finally, we use an LLM call to summarize all the code that has been generate and run to determine the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86324d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install e2b-code-interpreter\n",
    "%pip install anthropic\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the prompt that we will give to our agent.\n",
    "eda_prompt = f\"\"\"You are a Python data analyst. Your job is to analyze the `California-Wildfire-Data.csv`\n",
    "and generate a model to predict the damage done to houses based on the other features. The output of the Python code should\n",
    "provide useful insights. Your first task is to do an Exploritory Data Analysis on the data. You can assume that the \n",
    "csv is accessible to you at `/home/user/California-Wildfire-Data.csv`. \n",
    "\n",
    "**Task**\n",
    "For your first task, just load it and print the head.\n",
    "\n",
    "Packages you are allowed to use:\n",
    "- scikit-learn \n",
    "- pandas \n",
    "- numpy \n",
    "- seaborn \n",
    "- matplotlib\n",
    "\n",
    "**CRITICAL**: Return only python code, your text response will be immediately sent to run in a secure sandbox.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18fce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we send our first prompt to Claude, and it will generate our Python code for the EDA step.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "\n",
    "client = Anthropic(\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"), \n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    max_tokens=10000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": eda_prompt\n",
    "           ,\n",
    "        }\n",
    "    ],\n",
    "    model=\"claude-haiku-4-5-20251001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_code = message.content[0].text.lstrip(\"```python\").rstrip(\"```\")\n",
    "print(f\"Proposed code: \\n {eda_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe7db9",
   "metadata": {},
   "source": [
    "**Important note**\n",
    "E2B Sandboxes only run for 5 minutes by default, so the following cells should be run sequentially within 5 minutes, as they require memory of previous code to properly perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2b_code_interpreter import Sandbox\n",
    "\n",
    "sbx = Sandbox.create()\n",
    "sbx.commands.run(\"pip install scikit-learn pandas numpy seaborn matplotlib\")\n",
    "with open(\"California-Wildfire-Data.csv\", \"rb\") as file:\n",
    "\tsbx.files.write(\"/home/user/California-Wildfire-Data.csv\", file)\n",
    "result = sbx.run_code(eda_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ac9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_output = \"\"\n",
    "for elm in result.logs.stdout:\n",
    "    print(elm)\n",
    "    eda_output += elm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a798a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the prompt that we will give to our agent.\n",
    "cleaning_prompt = f\"\"\"You are a Python data analyst. A Jupyter notebook cell has just been run to load in and do an EDA of `California-Wildfire-Data.csv`.\n",
    "Based on the results of the code, please generate Python code to follow which will clean the data and report on what has been cleaned. Assume that your\n",
    "code will run in the next Jupyter notebook cell.\n",
    "\n",
    "**Task**\n",
    "Generate Python code to clean the data described by the code and output below.\n",
    "\n",
    "**Previous code block**\n",
    "{eda_code}\n",
    "\n",
    "**Output from that code**\n",
    "{eda_output}\n",
    "\n",
    "Packages you are allowed to use:\n",
    "- scikit-learn \n",
    "- pandas \n",
    "- numpy \n",
    "- seaborn \n",
    "- matplotlib\n",
    "\n",
    "**CRITICAL**: Return only python code, your text response will be immediately sent to run in a secure sandbox.\"\"\"\n",
    "\n",
    "message = client.messages.create(\n",
    "    max_tokens=10000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": cleaning_prompt\n",
    "           ,\n",
    "        }\n",
    "    ],\n",
    "    model=\"claude-haiku-4-5-20251001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_code = message.content[0].text.lstrip(\"```python\").rstrip(\"```\")\n",
    "print(f\"Proposed code: \\n {cleaning_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sbx.run_code(cleaning_code)\n",
    "cleaning_output = \"\"\n",
    "for elm in result.logs.stdout:\n",
    "    print(elm)\n",
    "    cleaning_output += elm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43676a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the prompt that we will give to our agent.\n",
    "model_prompt = f\"\"\"You are a Python data analyst. EDA and Data Cleaning have just been run on our dataset in `California-Wildfire-Data.csv`.\n",
    "Your job is to compare models based on their ability to predict Damage. You must generate Python code which should run each model and create\n",
    "a comprehensive report based on the results, detailing which model should be used and how (which parameters)\n",
    "\n",
    "**Task**\n",
    "Based on the EDA and Data Cleaning Results, run each of the models below on the data to find the best at predicting damage based on the other features.\n",
    "\n",
    "**Models**\n",
    "- CART\n",
    "- KNN\n",
    "- Naive Bayes\n",
    "\n",
    "**Previous code blocks**\n",
    "{eda_code}\n",
    "{cleaning_code}\n",
    "\n",
    "**Output from that code**\n",
    "{eda_output}\n",
    "{cleaning_code}\n",
    "\n",
    "Packages you are allowed to use:\n",
    "- scikit-learn \n",
    "- pandas \n",
    "- numpy \n",
    "- seaborn \n",
    "- matplotlib\n",
    "\n",
    "**CRITICAL**: Return only python code, your text response will be immediately sent to run in a secure sandbox.\"\"\"\n",
    "\n",
    "message = client.messages.create(\n",
    "    max_tokens=10000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": model_prompt\n",
    "           ,\n",
    "        }\n",
    "    ],\n",
    "    model=\"claude-haiku-4-5-20251001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = message.content[0].text.lstrip(\"```python\").rstrip(\"```\")\n",
    "print(f\"Proposed code: \\n {model_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sbx.run_code(model_code)\n",
    "model_output = \"\"\n",
    "for elm in result.logs.stdout:\n",
    "    print(elm)\n",
    "    model_output += elm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b249369",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = f\"\"\"You are a Python data analyst. You are receiving the results of a data analysis on comparing models, working with California\n",
    "Wildfire data. Based on the code and output, generate a markdown report summarizing the findings. Be concise, and be sure to explain which model \n",
    "performed the best, and why.\n",
    "\n",
    "**Task**\n",
    "Generate a markdown summary of the Python data analysis below\n",
    "\n",
    "**Previous code blocks**\n",
    "{eda_code}\n",
    "{cleaning_code}\n",
    "{model_code}\n",
    "\n",
    "**Output from that code**\n",
    "{eda_output}\n",
    "{cleaning_code}\n",
    "{model_output}\n",
    "\n",
    "**CRITICAL**: Return only the markdown report.\"\"\"\n",
    "\n",
    "message = client.messages.create(\n",
    "    max_tokens=10000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": summary_prompt\n",
    "           ,\n",
    "        }\n",
    "    ],\n",
    "    model=\"claude-haiku-4-5-20251001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295300f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(message.content[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
