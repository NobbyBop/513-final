{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dae31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and \"cleaning\" data\n",
    "df = pd.read_csv(\"California-Wildfire-Data.csv\")\n",
    "df_obj = df.select_dtypes(\"object\")\n",
    "\n",
    "df_obj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric data\n",
    "df_num = df.select_dtypes(\"number\")\n",
    "\n",
    "df_num[\"* Street Number\"] = df_num[\"* Street Number\"].replace(to_replace=0, value=np.nan)\n",
    "print(f\"Missing values in Street Number: {df_num[\"* Street Number\"].isna().sum()}\")\n",
    "\n",
    "df_num[\"Assessed Improved Value (parcel)\"] = df_num[\"Assessed Improved Value (parcel)\"].replace(to_replace=0, value=np.nan)\n",
    "print(f\"Missing values in Assessed Improved Value (parcel): {df_num[\"Assessed Improved Value (parcel)\"].isna().sum()}\")\n",
    "\n",
    "df_num[\"Year Built (parcel)\"] = df_num[\"Year Built (parcel)\"].replace(to_replace=0, value=np.nan)\n",
    "print(f\"Missing values in Year Built (parcel): {df_num[\"Year Built (parcel)\"].isna().sum()}\")\n",
    "\n",
    "#df_num = df_num.dropna()\n",
    "#print(f\"Rows remaining after dropping na: {len(df_num.index)}\")\n",
    "\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0df336",
   "metadata": {},
   "source": [
    "# Exploritory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zus7w80qn8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "for col in df_obj.columns:\n",
    "    unique_vals = df_obj[col].unique()\n",
    "    n_unique = len(unique_vals)\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Number of unique values: {n_unique}\")\n",
    "    if \"unknown\" in list(unique_vals):\n",
    "        print(f\"Number of missing values: {0 + df_obj[col].value_counts()[\"unknown\"]}\")\n",
    "    if \"Unknown\" in list(unique_vals):\n",
    "        print(f\"Number of missing values: {0 + df_obj[col].value_counts()[\"Unknown\"]}\")\n",
    "    if n_unique <= 20:\n",
    "        print(f\"Values: {list(unique_vals)}\")\n",
    "    else:\n",
    "        print(f\"Sample values (first 20): {list(unique_vals[:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_num.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Mean: {df_num[col].mean():.2f}\")\n",
    "    print(f\"Median: {df_num[col].median():.2f}\")\n",
    "    mode_vals = df_num[col].mode()\n",
    "    if len(mode_vals) > 0:\n",
    "        print(f\"Mode: {mode_vals[0]:.2f}\")\n",
    "    else:\n",
    "        print(f\"Mode: N/A\")\n",
    "    print(f\"Std Dev: {df_num[col].std():.2f}\")\n",
    "    print(f\"Min: {df_num[col].min():.2f}\")\n",
    "    print(f\"Max: {df_num[col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c3aca",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522eed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_encoded = df_obj.copy()\n",
    "le_target = LabelEncoder()\n",
    "\n",
    "# Fit and save the target encoder\n",
    "le_target.fit(df_obj['* Damage'].astype(str))\n",
    "\n",
    "# Encode all columns\n",
    "for col in df_encoded.columns:\n",
    "    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col].astype(str))\n",
    "\n",
    "# combine numeric and encoded categorical data, not feature selected yet\n",
    "attr = pd.concat([df_encoded.drop(['* Damage'], axis=1), df_num], axis=1)\n",
    "target = df_encoded['* Damage']\n",
    "\n",
    "attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=15)\n",
    "# optimal attributes/features\n",
    "out_feats = selector.fit_transform(attr.fillna(-1), target)\n",
    "opt_attr = pd.DataFrame(out_feats, columns=selector.get_feature_names_out()).dropna(axis=1)\n",
    "print(\"Best features:\", selector.get_feature_names_out())\n",
    "# Split data into training and testing sets 80-20\n",
    "attr_train, attr_test, target_train, target_test = train_test_split(opt_attr, target, test_size=0.2, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9682c",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "k_values = [1,3, 5, 7, 10, 25, 50]\n",
    "\n",
    "# for i in range(1,20):\n",
    "#     print(i)\n",
    "\n",
    "best_knn = None\n",
    "best_accuracy = 0\n",
    " \n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "#     knn = KNeighborsClassifier(n_neighbors = k,weights='distance')\n",
    "    knn.fit(attr_train, target_train)\n",
    "    target_pred = knn.predict(attr_test)\n",
    "#     accuracy = round(np.mean(target_test==target_pred ) * 100, 2)\n",
    "    accuracy = accuracy_score(target_test,target_pred ) \n",
    "    print(f'Testing accuracy of model with k = {k}: {accuracy}')\n",
    "    print('')\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_knn = knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a44b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "target_pred = best_knn.predict(attr_test)\n",
    "target_pred_dec = le_target.inverse_transform(target_pred)\n",
    "target_test_dec = le_target.inverse_transform(target_test)\n",
    "\n",
    "cm=confusion_matrix(target_test_dec, target_pred_dec)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(target_test_dec, target_pred_dec))\n",
    "print()\n",
    "print('Accuracy score')\n",
    "print(accuracy_score(target_test, target_pred))\n",
    "print()\n",
    "print('Classification Report')\n",
    "print(classification_report(target_test_dec, target_pred_dec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  # annot=True to annotate cells, fmt='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(le_target.inverse_transform(target.unique()))\n",
    "ax.yaxis.set_ticklabels(le_target.inverse_transform(target.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9d6fa",
   "metadata": {},
   "source": [
    "Looking at the accuracy, we can clearly see that KNN is doing well, with an 86%. However, we look at the confusion matrix and notice that it does not do as well for some of the categories, as is the case of the \"Destroyed\" category, where we seem to have trouble between telling no damage from destroyed. We seemingly also have a bit of trouble discerning \"No Damage\" from \"Inaccessible\". Some of this could be due to the inbalanced number of datapoints for each category in the dataset. The dataset has many more \"Inaccessible\" and \"No Damage\" homes than other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789ef99",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce34b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a label encoder here because using the get_dummies() method takes too much memory and crashes the kernel.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(attr_train, target_train)\n",
    "target_pred = model.predict(attr_test)\n",
    "\n",
    "# Decode predictions and targets\n",
    "target_test_decoded = le_target.inverse_transform(target_test)\n",
    "target_pred_decoded = le_target.inverse_transform(target_pred)\n",
    "\n",
    "# Get class labels\n",
    "labels = le_target.classes_\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(target_test_decoded, target_pred_decoded, labels=labels)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - CART')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(target_test_decoded, target_pred_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc43b2",
   "metadata": {},
   "source": [
    "Using the CART method, we see that our model is 87% accurate. We can also see that most of our data either falls into \"No Damage\" or \"Destroyed\". There is significantly less data for any other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39d58f",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e96c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "categorical_columns=['Latitude', 'Longitude', 'x', 'y']\n",
    "# Create preprocessor for categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', MinMaxScaler(), categorical_columns) ])\n",
    "# Create a Categorical Naive Bayes model\n",
    "cnb = CategoricalNB()\n",
    "\n",
    "# Create a pipeline with preprocessing and model\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', cnb)])\n",
    "# Create a Categorical Naive Bayes model\n",
    "\n",
    "# Train the model\n",
    "model.fit(attr_train, target_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "target_pred = model.predict(attr_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(target_test, target_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983cb3b",
   "metadata": {},
   "source": [
    "As we can see from the accuracy score above, naive bayes gives us a 54.73% accurate output, which is quite low when compared with other models. Thus, we conclude that Naive Bayes is unsuitable for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80bd5e",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d769a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22767b6",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb489530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc540d3",
   "metadata": {},
   "source": [
    "# HClust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedeb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af42bc",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
